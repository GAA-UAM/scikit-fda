@article{bagnall++_2018_uea,
  title = {The {{UEA}} Multivariate Time Series Classification Archive, 2018},
  author = {Bagnall, Anthony and Dau, Hoang Anh and Lines, Jason and Flynn, Michael and Large, James and Bostrom, Aaron and Southam, Paul and Keogh, Eamonn},
  year = {2018},
  month = oct,
  journal = {arXiv:1811.00075 [cs, stat]},
  eprint = {1811.00075},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.00075},
  urldate = {2018-11-16},
  abstract = {In 2002, the UCR time series classification archive was first released with sixteen datasets. It gradually expanded, until 2015 when it increased in size from 45 datasets to 85 datasets. In October 2018 more datasets were added, bringing the total to 128. The new archive contains a wide range of problems, including variable length series, but it still only contains univariate time series classification problems. One of the motivations for introducing the archive was to encourage researchers to perform a more rigorous evaluation of newly proposed time series classification (TSC) algorithms. It has worked: most recent research into TSC uses all 85 datasets to evaluate algorithmic advances. Research into multivariate time series classification, where more than one series are associated with each class label, is in a position where univariate TSC research was a decade ago. Algorithms are evaluated using very few datasets and claims of improvement are not based on statistical comparisons. We aim to address this problem by forming the first iteration of the MTSC archive, to be hosted at the website www.timeseriesclassification.com. Like the univariate archive, this formulation was a collaborative effort between researchers at the University of East Anglia (UEA) and the University of California, Riverside (UCR). The 2018 vintage consists of 30 datasets with a wide range of cases, dimensions and series lengths. For this first iteration of the archive we format all data to be of equal length, include no series with missing data and provide train/test splits.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{baillo+grane_2009_local,
  title = {Local Linear Regression for Functional Predictor and Scalar Response},
  author = {Ba{\'i}llo, Amparo and Gran{\'e}, Aurea},
  year = {2009},
  month = jan,
  journal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {1},
  pages = {102--111},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2008.03.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X08000973},
  urldate = {2023-04-03},
  abstract = {The aim of this work is to introduce a new nonparametric regression technique in the context of functional covariate and scalar response. We propose a local linear regression estimator and study its asymptotic behaviour. Its finite-sample performance is compared with a Nadayara\textendash Watson type kernel regression estimator and with the linear regression estimator via a Monte Carlo study and the analysis of two real data sets. In all the scenarios considered, the local linear regression estimator performs better than the kernel one, in the sense that the mean squared prediction error is lower.},
  langid = {english},
  keywords = {62G08,62G30,Cross-validation,Fourier expansion,Functional data,Kernel regression,Local linear regression,Nonparametric smoothing}
}

@article{berrendero++_2016_mrmr,
  title = {The {{mRMR}} Variable Selection Method: A Comparative Study for Functional Data},
  shorttitle = {The {{mRMR}} Variable Selection Method},
  author = {Berrendero, Jos{\'e} Ram{\'o}n and Cuevas, Antonio and Torrecilla, Jos{\'e} Luis},
  year = {2016},
  month = mar,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {86},
  number = {5},
  pages = {891--907},
  issn = {0094-9655},
  doi = {10.1080/00949655.2015.1042378},
  abstract = {The use of variable selection methods is particularly appealing in statistical problems with functional data. The obvious general criterion for variable selection is to choose the `most representative' or `most relevant' variables. However, it is also clear that a purely relevance-oriented criterion could lead to select many redundant variables. The minimum Redundance Maximum Relevance (mRMR) procedure, proposed by Ding and Peng [Minimum redundancy feature selection from microarray gene expression data. J Bioinform Comput Biol. 2005;3:185\textendash 205] and Peng et al. [Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy. IEEE Trans Pattern Anal Mach Intell. 2005;27:1226\textendash 1238] is an algorithm to systematically perform variable selection, achieving a reasonable trade-off between relevance and redundancy. In its original form, this procedure is based on the use of the so-called mutual information criterion to assess relevance and redundancy. Keeping the focus on functional data problems, we propose here a modified version of the mRMR method, obtained by replacing the mutual information by the new association measure (called distance correlation) suggested by Sz\'ekely et al. [Measuring and testing dependence by correlation of distances. Ann Statist. 2007;35:2769\textendash 2794]. We have also performed an extensive simulation study, including 1600 functional experiments (100 functional models \texttimes 4 sample sizes \texttimes 4 classifiers) and three real-data examples aimed at comparing the different versions of the mRMR methodology. The results are quite conclusive in favour of the new proposed alternative.},
  keywords = {distance correlation,functional data analysis,Primary: 62H30,Secondary: 62H20,supervised classification,variable selection}
}

@article{berrendero++_2016_variable,
  title = {Variable Selection in Functional Data Classification: A Maxima-Hunting Proposal},
  shorttitle = {Variable Selection in Functional Data Classification},
  author = {Berrendero, Jos{\'e} Ram{\'o}n and Cuevas, Antonio and Torrecilla, Jos{\'e} Luis},
  year = {2016},
  journal = {Statistica Sinica},
  volume = {26},
  number = {2},
  pages = {619--638},
  issn = {10170405},
  doi = {10.5705/ss.202014.0014},
  url = {http://www3.stat.sinica.edu.tw/statistica/J26N2/J26N210/J26N210.html},
  urldate = {2019-09-02},
  keywords = {distance correlation,functional data analysis,supervised classification,variable selection}
}

@article{berrendero++_2018_use,
  title = {On the Use of Reproducing Kernel {{Hilbert}} Spaces in Functional Classification},
  author = {Berrendero, Jos{\'e} Ram{\'o}n and Cuevas, Antonio and Torrecilla, Jos{\'e} Luis},
  year = {2018},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {523},
  pages = {1210--1218},
  issn = {0162-1459},
  doi = {10.1080/01621459.2017.1320287},
  abstract = {The H\'ajek\textendash Feldman dichotomy establishes that two Gaussian measures are either mutually absolutely continuous with respect to each other (and hence there is a Radon\textendash Nikodym density for each measure with respect to the other one) or mutually singular. Unlike the case of finite-dimensional Gaussian measures, there are nontrivial examples of both situations when dealing with Gaussian stochastic processes. This article provides: (a) Explicit expressions for the optimal (Bayes) rule and the minimal classification error probability in several relevant problems of supervised binary classification of mutually absolutely continuous Gaussian processes. The approach relies on some classical results in the theory of reproducing kernel Hilbert spaces (RKHS). (b) An interpretation, in terms of mutual singularity, for the so-called ``near perfect classification'' phenomenon. We show that the asymptotically optimal rule proposed by these authors can be identified with the sequence of optimal rules for an approximating sequence of classification problems in the absolutely continuous case. (c) As an application, we discuss a natural variable selection method, which essentially consists of taking the original functional data X(t), t {$\in$} [0, 1] to a d-dimensional marginal (X(t1), \ldots, X(td)), which is chosen to minimize the classification error of the corresponding Fisher's linear rule. We give precise conditions under which this discrimination method achieves the minimal classification error of the original functional problem. Supplementary materials for this article are available online.},
  keywords = {absolutely continuity,mutually singular processes,Radonâ€“Nikodym derivatives,supervised functional classification,variable selection}
}

@article{berrendero++_2020_mahalanobis,
  title = {On {{Mahalanobis}} Distance in Functional Settings},
  author = {Berrendero, Jos{\'e} R. and {Bueno-Larraz}, Beatriz and Cuevas, Antonio},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {9},
  pages = {1--33},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v21/18-156.html},
  urldate = {2022-06-29},
  abstract = {Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.},
  keywords = {Binary classification,Hilbert space,Population Parameter,Sampling (signal processing),Smoothing (statistical technique),Stochastic process}
}

@article{berrendero++_2022_functional,
  title = {On Functional Logistic Regression: Some Conceptual Issues},
  shorttitle = {On Functional Logistic Regression},
  author = {Berrendero, Jos{\'e} R. and {Bueno-Larraz}, Beatriz and Cuevas, Antonio},
  year = {2022},
  month = oct,
  journal = {TEST},
  issn = {1863-8260},
  doi = {10.1007/s11749-022-00836-9},
  abstract = {The main ideas behind the classic multivariate logistic regression model make sense when translated to the functional setting, where the explanatory variable X is a function and the response Y is binary. However, some important technical issues appear (or are aggravated with respect to those of the multivariate case) due to the functional nature of the explanatory variable. First, the mere definition of the model can be questioned: While most approaches so far proposed rely on the \$\$L\^2\$\$-based model, we explore an alternative (in some sense, more general) approach, based on the theory of reproducing kernel Hilbert spaces (RKHS). The validity conditions of such RKHS-based model, and their relation with the \$\$L\^2\$\$-based one, are investigated and made explicit in two formal results. Some relevant particular cases are considered as well. Second, we show that, under very general conditions, the maximum likelihood of the logistic model parameters fails to exist in the functional case, although some restricted versions can be considered. Third, we check (in the framework of binary classification) the practical performance of some RKHS-based procedures, well-suited to our model: They are compared to several competing methods via Monte Carlo experiments and the analysis of real data sets.},
  langid = {english},
  keywords = {62J12,62R10,Functional data,Kernel methods in statistics,Logistic regression,Mathematics - Statistics Theory,Reproducing kernel Hilbert spaces}
}

@article{cuesta-albertos++_2017_ddgclassifier,
  title = {The {{DDá´³}}-Classifier in the Functional Setting},
  author = {{Cuesta-Albertos}, J. A. and {Febrero-Bande}, M. and {Oviedo~de~la~Fuente}, M.},
  year = {2017},
  month = mar,
  journal = {TEST},
  volume = {26},
  number = {1},
  pages = {119--142},
  issn = {1863-8260},
  doi = {10.1007/s11749-016-0502-6},
  abstract = {The maximum depth classifier was the first attempt to use data depths instead of multivariate raw data in classification problems. Recently, the DD-classifier has addressed some of the serious limitations of this classifier but issues still remain. This paper aims to extend the DD-classifier as follows: first, by enabling it to handle more than two groups; second, by applying regular classification methods (such as kNN, linear or quadratic classifiers, recursive partitioning, etc) to DD-plots, which is particularly useful, because it gives insights based on the diagnostics of these methods; and third, by integrating various sources of information (data depths, multivariate functional data, etc) in the classification procedure in a unified way. This paper also proposes an enhanced revision of several functional data depths and it provides a simulation study and applications to some real data sets.},
  langid = {english},
  keywords = {62-09,62G99,62H30,DD-classifier,Functional data analysis,Functional depths}
}

@article{cuevas++_2004_anova,
  title = {An {{ANOVA}} Test for Functional Data},
  author = {Cuevas, Antonio and Febrero, Manuel and Fraiman, Ricardo},
  year = {2004},
  month = aug,
  journal = {Computational Statistics \& Data Analysis},
  volume = {47},
  number = {1},
  pages = {111--122},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2003.10.021},
  url = {http://www.sciencedirect.com/science/article/pii/S016794730300269X},
  urldate = {2018-12-10},
  abstract = {Given k independent samples of functional data the problem of testing the null hypothesis of equality of their respective mean functions is considered. So the setting is quite similar to that of the classical one-way anova model but the k samples under study consist of functional data. A simple natural test for this problem is proposed. It can be seen as an asymptotic version of the well-known anova F-test. The asymptotic validity of the method is shown. A numerical Monte Carlo procedure is proposed to handle in practice the asymptotic distribution of the test statistic. A simulation study is included and a real-data example in experimental cardiology is considered in some detail.},
  keywords = {Comparison of curves,Equality of functional means,Functional data,Functional one-way anova,Longitudinal data}
}

@article{dai+genton_2018_multivariate,
  title = {Multivariate Functional Data Visualization and Outlier Detection},
  author = {Dai, Wenlin and Genton, Marc G.},
  year = {2018},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {27},
  number = {4},
  pages = {923--934},
  issn = {1061-8600},
  doi = {10.1080/10618600.2018.1473781},
  abstract = {This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate nonoutlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data. Supplementary material for this article is available online.},
  keywords = {Data visualization,Directional outlyingness,Functional data,Graphical tool,Magnitude and shape,Outlier detection,Statistics - Computation,Statistics - Methodology}
}

@article{dau++_2019_ucr,
  title = {The {{UCR}} Time Series Archive},
  author = {Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Keogh, Eamonn},
  year = {2019},
  month = nov,
  journal = {IEEE/CAA Journal of Automatica Sinica},
  volume = {6},
  number = {6},
  pages = {1293--1305},
  issn = {2329-9274},
  doi = {10.1109/JAS.2019.1911747},
  abstract = {The UCR time series archive - introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one data set from the archive. The original incarnation of the archive had sixteen data sets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 to 85 data sets. This paper introduces and will focus on the new data expansion from 85 to 128 data sets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline (1nearest neighbor classification), a fraction might be mis-attributing the reasons for their improvement. Moreover, the improvements claimed by these papers might have been achievable with a much simpler modification, requiring just a few lines of code.},
  keywords = {Cameras,Computer Science - Machine Learning,data mining,Data mining,Error analysis,Euclidean distance,Microsoft Windows,Statistics - Machine Learning,Time series analysis,time series classification,Training,UCR time series archive}
}

@article{febrero++_2008_outlier,
  title = {Outlier Detection in Functional Data by Depth Measures, with Application to Identify Abnormal {{NOx}} Levels},
  author = {Febrero, Manuel and Galeano, Pedro and Gonz{\'a}lez-Manteiga, Wenceslao},
  year = {2008},
  month = jun,
  journal = {Environmetrics},
  volume = {19},
  number = {4},
  pages = {331--345},
  issn = {1099-095X},
  doi = {10.1002/env.878},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.878},
  urldate = {2018-12-10},
  abstract = {This paper analyzes outlier detection for functional data by means of functional depths, which measures the centrality of a given curve within a group of trajectories providing center-outward orderings of the set of curves. We give some insights of the usefulness of looking for outliers in functional datasets and propose a method based in depths for the functional outlier detection. The performance of the proposed procedure is analyzed by several Monte Carlo experiments. Finally, we illustrate the procedure by finding outliers in a dataset of NOx (nitrogen oxides) emissions taken from a control station near an industrial area. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {depths,functional median,functional trimmed mean,nitrogen oxides,outliers,smoothed bootstrap}
}

@article{febrero-bande+oviedodelafuente_2012_statistical,
  title = {Statistical Computing in Functional Data Analysis: The {{R}} Package Fda.Usc},
  shorttitle = {Statistical Computing in Functional Data Analysis},
  author = {{Febrero-Bande}, Manuel and {Oviedo de la Fuente}, Manuel},
  year = {2012},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {51},
  number = {1},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v051.i04},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v051i04},
  urldate = {2019-10-01},
  copyright = {Copyright (c) 2011 Manuel Febrero-Bande, Manuel Oviedo de la Fuente},
  langid = {english},
  keywords = {depth measures,functional data regression,non-parametric kernel estimation,outlier,representation of functional data}
}

@inbook{ferraty+vieu_2006_computational,
  title = {Computational Issues},
  booktitle = {Nonparametric Functional Data Analysis: Theory and Practice},
  author = {Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
  year = {2006},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {99--108},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  url = {https://www.springer.com/gp/book/9780387303697},
  urldate = {2019-09-10},
  collaborator = {Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
  isbn = {978-0-387-30369-7},
  langid = {english}
}

@inbook{ferraty+vieu_2006_functional,
  title = {Functional Nonparametric Prediction Methodologies},
  booktitle = {Nonparametric Functional Data Analysis: Theory and Practice},
  author = {Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
  year = {2006},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {49--59},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  url = {https://www.springer.com/gp/book/9780387303697},
  urldate = {2019-09-10},
  collaborator = {Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
  isbn = {978-0-387-30369-7},
  langid = {english}
}

@article{fraiman+muniz_2001_trimmed,
  title = {Trimmed Means for Functional Data},
  author = {Fraiman, Ricardo and Muniz, Graciela},
  year = {2001},
  month = dec,
  journal = {Test},
  volume = {10},
  number = {2},
  pages = {419--440},
  issn = {1863-8260},
  doi = {10.1007/BF02595706},
  url = {https://doi.org/10.1007/BF02595706},
  urldate = {2020-10-19},
  abstract = {In practice, the use of functional data is often preferable to that of large finitedimensional vectors obtained by discrete approximations of functions. In this paper a new concept of data depth is introduced for functional data. The aim is to measure the centrality of a given curve within a group of curves. This concept is used to define ranks and trimmed means for functional data. Some theoretical and practical aspects are discussed and a simulation study is given. The results show a good performance of our method, in terms of efficiency and robustness, when compared with the mean. Finally, a real-data example based on the Nasdaq 100 index is discussed.},
  langid = {english},
  keywords = {data depth,functional data,trimmed means estimates}
}

@article{gervini_2008_robust,
  title = {Robust Functional Estimation Using the Median and Spherical Principal Components},
  author = {Gervini, Daniel},
  year = {2008},
  month = sep,
  journal = {Biometrika},
  volume = {95},
  number = {3},
  pages = {587--600},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/asn031},
  url = {https://academic.oup.com/biomet/article/95/3/587/217516},
  urldate = {2020-11-29},
  abstract = {Abstract. We present robust estimators for the mean and the principal components of a stochastic process in . Robustness and asymptotic properties of the estima},
  langid = {english}
}

@article{ghosh+chaudhuri_2005_maximum,
  title = {On Maximum Depth and Related Classifiers},
  author = {Ghosh, Anil K. and Chaudhuri, Probal},
  year = {2005},
  journal = {Scandinavian Journal of Statistics},
  volume = {32},
  number = {2},
  pages = {327--350},
  issn = {1467-9469},
  doi = {10.1111/j.1467-9469.2005.00423.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2005.00423.x},
  urldate = {2020-01-10},
  abstract = {Abstract. Over the last couple of decades, data depth has emerged as a powerful exploratory and inferential tool for multivariate data analysis with wide-spread applications. This paper investigates the possible use of different notions of data depth in non-parametric discriminant analysis. First, we consider the situation where the prior probabilities of the competing populations are all equal and investigate classifiers that assign an observation to the population with respect to which it has the maximum location depth. We propose a different depth-based classification technique for unequal prior problems, which is also useful for equal prior cases, especially when the populations have different scatters and shapes. We use some simulated data sets as well as some benchmark real examples to evaluate the performance of these depth-based classifiers. Large sample behaviour of the misclassification rates of these depth-based non-parametric classifiers have been derived under appropriate regularity conditions.},
  langid = {english},
  keywords = {Bayes risk,cross-validation,data depth,elliptic symmetry,kernel density estimation,location shift model,Mahalanobis distance,misclassification rate,Vapnik Chervonenkis dimension}
}

@article{li++_2012_ddclassifier,
  title = {{{DD-Classifier}}: {{Nonparametric}} Classification Procedure Based on {{DD-Plot}}},
  shorttitle = {{{DD-Classifier}}},
  author = {Li, Jun and {Cuesta-Albertos}, Juan A. and Liu, Regina Y.},
  year = {2012},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {107},
  number = {498},
  pages = {737--753},
  issn = {0162-1459},
  doi = {10.1080/01621459.2012.688462},
  url = {https://doi.org/10.1080/01621459.2012.688462},
  urldate = {2020-01-10},
  abstract = {Using the DD-plot (depth vs. depth plot), we introduce a new nonparametric classification algorithm and call it DD-classifier. The algorithm is completely nonparametric, and it requires no prior knowledge of the underlying distributions or the form of the separating curve. Thus, it can be applied to a wide range of classification problems. The algorithm is completely data driven and its classification outcome can be easily visualized in a two-dimensional plot regardless of the dimension of the data. Moreover, it has the advantage of bypassing the estimation of underlying parameters such as means and scales, which is often required by the existing classification procedures. We study the asymptotic properties of the DD-classifier and its misclassification rate. Specifically, we show that DD-classifier is asymptotically equivalent to the Bayes rule under suitable conditions, and it can achieve Bayes error for a family broader than elliptical distributions. The performance of the classifier is also examined using simulated and real datasets. Overall, the DD-classifier performs well across a broad range of settings, and compares favorably with existing classifiers. It can also be robust against outliers or contamination.},
  keywords = {Classification,Data depth,DD-classifier,DD-plot,Maximum depth classifier,Misclassification rates,Nonparametric,Robustness}
}

@article{malfait+ramsay_2003_historical,
  title = {The Historical Functional Linear Model},
  author = {Malfait, Nicole and Ramsay, James O.},
  year = {2003},
  journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
  volume = {31},
  number = {2},
  eprint = {3316063},
  eprinttype = {jstor},
  pages = {115--128},
  publisher = {{[Statistical Society of Canada, Wiley]}},
  issn = {0319-5724},
  doi = {10.2307/3316063},
  url = {https://www.jstor.org/stable/3316063},
  urldate = {2022-07-21},
  abstract = {The authors develop a functional linear model in which the values at time t of a sample of curves yi(t) are explained in a feed-forward sense by the values of covariate curves xi(s) observed at times s {$\leq$} t. They give special attention to the case s {$\in$} [t - {$\delta$}, t], where the lag parameter {$\delta$} is estimated from the data. They use the finite element method to estimate the bivariate parameter regression function {$\beta$}(s, t), which is defined on the triangular domain s {$\leq$} t. They apply their model to the problem of predicting the acceleration of the lower lip during speech on the basis of electromyographical recordings from a muscle depressing the lip. They also provide simulation results to guide the calibration of the fitting process. /// Les auteurs d\'ecrivent un mod\`ele lin\'eaire fonctionnel dans lequel les valeurs au temps t d'un \'echantillon de courbes yi(t) sont expliqu\'ees par les valeurs observ\'ees aux temps s {$\leq$} t de courbes covariables xi(s). Ils accordent une attention particuli\`ere au cas o\`u s {$\in$} [t - {$\delta$}, t], {$\delta$} repr\'esentant un param\`etre de d\'elai estim\'e \`a partir des donn\'ees. Ils emploient la m\'ethode des \'el\'ements finis pour estimer la fonction param\`etre {$\beta$}(s, t) bivari\'ee d\'efinie sur le domaine triangulaire s {$\leq$} t. Ils appliquent leur mod\`ele \`a la pr\'evision de courbes d'acc\'el\'eration de la l\`evre inf\'erieure d'un locuteur \`a partir d'enregistrements \'electromyographiques d'un muscle abaissant celle-ci. Ils pr\'esentent aussi des r\'esultats de simulation pouvant guider le processus de calibration intervenant dans l'ajustement du mod\`ele.}
}

@article{marron++_2015_functional,
  title = {Functional Data Analysis of Amplitude and Phase Variation},
  author = {Marron, J. S. and Ramsay, James O. and Sangalli, Laura M. and Srivastava, Anuj},
  year = {2015},
  month = nov,
  journal = {Statistical Science},
  volume = {30},
  number = {4},
  pages = {468--484},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/15-STS524},
  url = {https://projecteuclid.org/journals/statistical-science/volume-30/issue-4/Functional-Data-Analysis-of-Amplitude-and-Phase-Variation/10.1214/15-STS524.full},
  urldate = {2023-01-29},
  abstract = {The abundance of functional observations in scientific endeavors has led to a significant development in tools for functional data analysis (FDA). This kind of data comes with several challenges: infinite-dimensionality of function spaces, observation noise, and so on. However, there is another interesting phenomena that creates problems in FDA. The functional data often comes with lateral displacements/deformations in curves, a phenomenon which is different from the height or amplitude variability and is termed phase variation. The presence of phase variability artificially often inflates data variance, blurs underlying data structures, and distorts principal components. While the separation and/or removal of phase from amplitude data is desirable, this is a difficult problem. In particular, a commonly used alignment procedure, based on minimizing the \$\textbackslash mathbb\{L\}\^\{2\}\$ norm between functions, does not provide satisfactory results. In this paper we motivate the importance of dealing with the phase variability and summarize several current ideas for separating phase and amplitude components. These approaches differ in the following: (1) the definition and mathematical representation of phase variability, (2) the objective functions that are used in functional data alignment, and (3) the algorithmic tools for solving estimation/optimization problems. We use simple examples to illustrate various approaches and to provide useful contrast between them.},
  keywords = {alignment,dynamic time warping,elastic metric,Fisherâ€“Rao metric,Functional data analysis,registration,warping}
}

@article{pini++_2018_hotelling,
  title = {Hotelling's {{T2}} in Separable {{Hilbert}} Spaces},
  author = {Pini, Alessia and Stamm, Aymeric and Vantini, Simone},
  year = {2018},
  month = sep,
  journal = {Journal of Multivariate Analysis},
  volume = {167},
  pages = {284--305},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2018.05.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X17302646},
  urldate = {2023-04-03},
  abstract = {We address the problem of finite-sample null hypothesis significance testing on the mean element of a random variable that takes value in a generic separable Hilbert space. For this purpose, we propose a (re)definition of Hotelling's T2 that naturally expands to any separable Hilbert space that we further embed within a permutation inferential approach. In detail, we present a unified framework for making inference on the mean element of Hilbert populations based on Hotelling's T2 statistic, using a permutation-based testing procedure of which we prove finite-sample exactness and consistency; we showcase the explicit form of Hotelling's T2 statistic in the case of some famous spaces used in functional data analysis (i.e., Sobolev and Bayes spaces); we demonstrate, by means of simulations, that Hotelling's T2 exhibits the best performances in terms of statistical power for detecting mean differences between Gaussian populations, compared to other state-of-the-art statistics, in most simulated scenarios; we propose a case study that demonstrate the importance of the space into which one decides to embed the data; we provide an implementation of the proposed tools in the R package fdahotelling available at https://github.com/astamm/fdahotelling.},
  langid = {english},
  keywords = {Functional data,High-dimensional data Hotellingâ€™s,Hilbert space,Nonparametric inference,Permutation test}
}

@inbook{ramsay+silverman_2005_functionala,
  title = {From Functional Data to Smooth Functions},
  booktitle = {Functional Data Analysis},
  author = {Ramsay, James and Silverman, Bernard W.},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  pages = {37--58},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98888},
  url = {https://www.springer.com/gp/book/9780387400808},
  urldate = {2021-09-19},
  collaborator = {Ramsay, James and Silverman, Bernard W.},
  isbn = {978-0-387-40080-8},
  langid = {english},
  keywords = {Multivariate analysis}
}

@inbook{ramsay+silverman_2005_introduction,
  title = {Introduction},
  booktitle = {Functional Data Analysis},
  author = {Ramsay, James and Silverman, Bernard W.},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  pages = {1--18},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98888},
  url = {https://www.springer.com/gp/book/9780387400808},
  urldate = {2021-09-19},
  collaborator = {Ramsay, James and Silverman, Bernard W.},
  isbn = {978-0-387-40080-8},
  langid = {english},
  keywords = {Multivariate analysis}
}

@inbook{ramsay+silverman_2005_registration,
  title = {The Registration and Display of Functional Data},
  booktitle = {Functional Data Analysis},
  author = {Ramsay, James and Silverman, Bernard W.},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  pages = {127--145},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98888},
  url = {https://www.springer.com/gp/book/9780387400808},
  urldate = {2021-09-19},
  collaborator = {Ramsay, James and Silverman, Bernard W.},
  isbn = {978-0-387-40080-8},
  langid = {english},
  keywords = {Multivariate analysis}
}

@article{ruiz-meana++_2003_cariporide,
  title = {Cariporide Preserves Mitochondrial Proton Gradient and Delays {{ATP}} Depletion in Cardiomyocytes during Ischemic Conditions},
  author = {{Ruiz-Meana}, Marisol and {Garcia-Dorado}, David and Pina, Pilar and Inserte, Javier and Agull{\'o}, Luis and {Soler-Soler}, Jordi},
  year = {2003},
  month = sep,
  journal = {American Journal of Physiology. Heart and Circulatory Physiology},
  volume = {285},
  number = {3},
  pages = {H999-1006},
  issn = {0363-6135},
  doi = {10.1152/ajpheart.00035.2003},
  abstract = {The mechanism by which inhibition of Na+/H+ exchanger (NHE) reduces cell death in ischemic-reperfused myocardium remains controversial. This study investigated whether cariporide could inhibit mitochondrial NHE during ischemia, delaying H+ gradient dissipation and ATP exhaustion. Mouse cardiac myocytes (HL-1) were submitted to 1 h of simulated ischemia (SI) with NaCN/deoxyglucose (pH 6.4), with or without 7 microM cariporide, and mitochondrial concentration of Ca2+ (Rhod-2), 2', 7'-bis(2-carboxyethyl)-5(6)-carboxyfluorescein (BCECF) and the charge difference across the mitochondrial membrane potential (Deltapsim, JC-1) were assessed. ATP content was measured by bioluminescence and mitochondrial swelling by spectrophotometry in isolated mitochondria. Cariporide significantly attenuated the acidification of the mitochondrial matrix induced by SI without modifying Deltapsim decay, and this effect was associated to a delayed ATP exhaustion and increased mitochondrial Ca2+ load. These effects were reproduced in sarcolemma-permeabilized cells exposed to SI. In these cells, cariporide markedly attenuated the fall in mitochondrial pH induced by removal of Na+ from the medium. In isolated mitochondria, cariporide significantly reduced the rate and magnitude of passive matrix swelling induced by Na+ acetate. In isolated rat hearts submitted to 40-min ischemia at different temperatures (35.5 degrees, 37 degrees, or 38.5 degrees C) pretreatment with cariporide limited ATP depletion during the first 10 min of ischemia and cell death (lactate dehydrogenase release) during reperfusion. These effects were mimicked when a similar ATP preservation was achieved by hypothermia and were abolished when the sparing effect of cariporide on ATP was suppressed by hyperthermia. We conclude that cariporide acts at the mitochondrial level, delaying mitochondrial matrix acidification and delaying ATP exhaustion during ischemia. These effects can contribute to reduce cell death secondary to ischemia-reperfusion.},
  langid = {english},
  pmid = {12915386},
  keywords = {Acids,Adenosine Triphosphate,Animals,Anti-Arrhythmia Agents,Cardiotonic Agents,Cell Death,Cells; Cultured,Energy Metabolism,Guanidines,Mice,Mitochondria,Myocardial Infarction,Myocardial Reperfusion Injury,Myocytes; Cardiac,Protons,Sodium-Hydrogen Exchangers,Sulfones}
}

@article{serfling+zuo_2000_general,
  title = {General Notions of Statistical Depth Function},
  author = {Serfling, Robert and Zuo, Yijun},
  year = {2000},
  journal = {The Annals of Statistics},
  volume = {28},
  number = {2},
  pages = {461--482},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1016218226},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/General-notions-of-statistical-depth-function/10.1214/aos/1016218226.full},
  urldate = {2022-07-01},
  abstract = {Statistical depth functions are being formulated ad hoc with increasing popularity in nonparametric inference for multivariate data. Here we introduce several general structures for depth functions, classify many existing examples as special cases, and establish results on the possession, or lack thereof, of four key properties desirable for depth functions in general. Roughly speaking, these properties may be described as: affine invariance, maximality at center, monotonicity relative to deepest point, and vanishing at infinity. This provides a more systematic basis for selection of a depth function. In particular, from these and other considerations it is found that the halfspace depth behaves very well overall in comparison with various competitors.},
  keywords = {62G20,62H05,halfspace depth,multivariate symmetry,simplicial depth,Statistical depth functions}
}

@inbook{srivastava+klassen_2016_functionala,
  title = {Functional Data and Elastic Registration},
  booktitle = {Functional and Shape Data Analysis},
  author = {Srivastava, Anuj and Klassen, Eric P.},
  editor = {Bicke, Peter and Diggle, Peter and Fienberg, Stephen E. and Gather, Ursula and Olkin, Ingram and Zeger, Scott},
  year = {2016},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {73--123},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4939-4020-2},
  url = {https://www.springer.com/gp/book/9781493940189},
  urldate = {2020-01-12},
  isbn = {978-1-4939-4018-9},
  langid = {english}
}

@inbook{srivastava+klassen_2016_statistical,
  title = {Statistical Modeling of Functional Data},
  booktitle = {Functional and Shape Data Analysis},
  author = {Srivastava, Anuj and Klassen, Eric P.},
  editor = {Bicke, Peter and Diggle, Peter and Fienberg, Stephen E. and Gather, Ursula and Olkin, Ingram and Zeger, Scott},
  year = {2016},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {269--303},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4939-4020-2},
  url = {https://www.springer.com/gp/book/9781493940189},
  urldate = {2020-01-12},
  isbn = {978-1-4939-4018-9},
  langid = {english}
}

@misc{srivastava++_2011_registration,
  title = {Registration of Functional Data {{Using Fisher-Rao}} Metric},
  author = {Srivastava, Anuj and Wu, Wei and Kurtek, Sebastian and Klassen, Eric and Marron, J. S.},
  year = {2011},
  month = may,
  number = {arXiv:1103.3817},
  eprint = {arXiv:1103.3817},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1103.3817},
  url = {http://arxiv.org/abs/1103.3817},
  urldate = {2022-09-29},
  abstract = {We introduce a novel geometric framework for separating the phase and the amplitude variability in functional data of the type frequently studied in growth curve analysis. This framework uses the Fisher-Rao Riemannian metric to derive a proper distance on the quotient space of functions modulo the time-warping group. A convenient square-root velocity function (SRVF) representation transforms the Fisher-Rao metric into the standard \$\textbackslash ltwo\$ metric, simplifying the computations. This distance is then used to define a Karcher mean template and warp the individual functions to align them with the Karcher mean template. The strength of this framework is demonstrated by deriving a consistent estimator of a signal observed under random warping, scaling, and vertical translation. These ideas are demonstrated using both simulated and real data from different application domains: the Berkeley growth study, handwritten signature curves, neuroscience spike trains, and gene expression signals. The proposed method is empirically shown to be be superior in performance to several recently published methods for functional alignment.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology}
}

@article{sun+genton_2011_functional,
  title = {Functional Boxplots},
  author = {Sun, Ying and Genton, Marc G.},
  year = {2011},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {2},
  pages = {316--334},
  issn = {1061-8600},
  doi = {10.1198/jcgs.2011.09224},
  urldate = {2018-09-26},
  abstract = {This article proposes an informative exploratory tool, the functional boxplot, for visualizing functional data, as well as its generalization, the enhanced functional boxplot. Based on the center outward ordering induced by band depth for functional data, the descriptive statistics of a functional boxplot are: the envelope of the 50\% central region, the median curve, and the maximum non-outlying envelope. In addition, outliers can be detected in a functional boxplot by the 1.5 times the 50\% central region empirical rule, analogous to the rule for classical boxplots. The construction of a functional boxplot is illustrated on a series of sea surface temperatures related to the El Ni\~no phenomenon and its outlier detection performance is explored by simulations. As applications, the functional boxplot and enhanced functional boxplot are demonstrated on children growth data and spatio-temporal U.S. precipitation data for nine climatic regions, respectively. This article has supplementary material online.},
  keywords = {Depth,Functional data,Growth data,Precipitation data,Spaceâ€“time data,Visualization}
}

@article{szekely+rizzo_2009_brownian,
  title = {Brownian Distance Covariance},
  author = {Sz{\'e}kely, G{\'a}bor J. and Rizzo, Maria L.},
  year = {2009},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {3},
  number = {4},
  pages = {1236--1265},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/09-AOAS312},
  url = {https://projecteuclid.org/euclid.aoas/1267453933},
  urldate = {2018-09-25},
  abstract = {Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.},
  langid = {english},
  mrnumber = {MR2752127},
  zmnumber = {1196.62077},
  keywords = {Brownian covariance,dcor,distance correlation,independence,multivariate}
}

@inproceedings{torrecilla+suarez_2016_feature,
  title = {Feature Selection in Functional Data Classification with Recursive Maxima Hunting},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Torrecilla, Jos{\'e} Luis and Su{\'a}rez, Alberto},
  year = {2016},
  pages = {4835--4843},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6392-feature-selection-in-functional-data-classification-with-recursive-maxima-hunting.pdf},
  urldate = {2018-09-25}
}

@inbook{wasserman_2006_nonparametric,
  title = {Nonparametric Regression},
  booktitle = {All of Nonparametric Statistics},
  author = {Wasserman, Larry},
  year = {2006},
  series = {Springer {{Texts}} in {{Statistics}}},
  pages = {61--123},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  url = {https://www.springer.com/gp/book/9780387251455},
  urldate = {2019-06-25},
  collaborator = {Wasserman, Larry},
  isbn = {978-0-387-25145-5},
  langid = {english}
}

